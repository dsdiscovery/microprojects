{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "<div style=\"color: #DD3403; font-size: 60%\">Data Science DISCOVERY MicroProject</div>\n",
    "<span style=\"\">MicroProject: Illini Football</span>\n",
    "<div style=\"font-size: 60%;\"><a href=\"https://discovery.cs.illinois.edu/microproject/illini-football-scores/\">https://discovery.cs.illinois.edu/microproject/illini-football-scores/</a></div>\n",
    "</h1>\n",
    "\n",
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset: \"illini-football\"\n",
    "\n",
    "The [University of Illinois' Fighting Illini Historical Football Scores Dataset](https://github.com/wadefagen/datasets/tree/master/illini-football) is a dataset maintained as part of [@wadefagen](https://github.com/wadefagen)'s datasets GitHub repository.  This dataset contains a \"collection of final scores of every known Fighting Illini football game since 1892, with data on location, homecoming, and national bowl games.\"\n",
    "\n",
    "The URL for the CSV dataset is:\n",
    "```\n",
    "https://raw.githubusercontent.com/wadefagen/datasets/master/illini-football/illini-football-scores.csv\n",
    "```\n",
    "\n",
    "Load the dataset into a DataFrame called `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Puzzle 1: The Best and Worst Games in History\n",
    "\n",
    "Since 1892, The University of Illinois has played over 1,200 games of football against 130 different schools!  This dataset reports the final score of the game, with a column for the score of the Illini and a column for score of the Opponent.\n",
    "\n",
    "In the first puzzle, let's find a few interesting games!\n",
    "\n",
    "\n",
    "### The Worst Game of Illini Football\n",
    "\n",
    "Let's consider the \"worst\" game to be the game where the Illini lost by the most points. *(This is not the game where the opponent scored the most points, but the game where there score had the biggest difference.)\n",
    "\n",
    "Create a DataFrame df_worst that contains the rows of the greatest score difference. There might be more than one row with the same score difference, so you should use `df.nlargest()` or `df.nsmallest()` to get the rows with the largest score difference. To not drop the duplicates, you can use `keep='all'` as an argument to `df.nlargest()` or `df.nsmallest()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worst = ...\n",
    "df_worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The Craziest Game of Illini Football\n",
    "\n",
    "Let's consider the \"craziest\" game of Illini Football the game where the **most total points** were scored.\n",
    "\n",
    "Create a DataFrame `df_craziest` that contains just one row of the very craziest game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_craziest = ...\n",
    "df_craziest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The Best Homecoming Game\n",
    "\n",
    "In football, the \"homecoming game\" is a football game played at the team's home stadium and is often accompanied with celebrations all week (\"Homecoming Week\").  In this dataset, homecoming games are denoted by the `\"Note\"` column containing the string `\"Homecoming\"`.\n",
    "\n",
    "Find the \"best\" homecoming game, where the Illini won by the most points.  Create a DataFrame `df_homecoming_best` that contains just one row of the best homecoming game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_homecoming_best = ...\n",
    "df_homecoming_best"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ”¬ MicroProject Checkpoint Tests ðŸ”¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 1: The Best and Worst Games in History\n",
    "#\n",
    "# What is this cell?\n",
    "# - This cell contains test cases for the MicroProject. Even though you can modify this\n",
    "#   cell, you should treat it like it's a read-only cell since it will be replaced with\n",
    "#   a fresh version when your code is checked.\n",
    "#\n",
    "# - If this cell runs without any error in the output, you PASSED all test cases!\n",
    "#   We try and make these test cases as useful and complete as possible, but there is\n",
    "#   a chance your code may be incorrect even though you pass the test cases (these\n",
    "#   tests should be seen as a way to give you confidence that code you believe is\n",
    "#   actually correct, not as a robust check to catch all possible errors).\n",
    "#\n",
    "# - If this cell results in any errors, check you previous cells, make changes, and\n",
    "#   RE-RUN your code and then re-run this cell.  Keep repeating this until the cell\n",
    "#   passed with no errors! :)\n",
    "\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "assert( 'df' in vars() ), \"You appear to not have the correct dataset loaded in `df`.\"\n",
    "assert( len(df) > 1200 ), \"You appear to not have the correct dataset loaded in `df`.\"\n",
    "assert( \"IlliniScore\" in df ), \"You appear to not have the correct dataset loaded in `df`.\"\n",
    "assert( \"OpponentScore\" in df ), \"You appear to not have the correct dataset loaded in `df`.\"\n",
    "\n",
    "assert( 'df_worst' in vars() ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( len(df_worst) == 2 ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( 'df_craziest' in vars() ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( len(df_craziest) == 1 ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( 'df_homecoming_best' in vars() ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( len(df_homecoming_best) == 1 ), \"Puzzle #1 appears incorrect.\"\n",
    "\n",
    "OS = list(df.columns).index(\"OpponentScore\")\n",
    "IS = list(df.columns).index(\"IlliniScore\")\n",
    "__tdf = df.dropna(subset=[\"IlliniScore\"])\n",
    "__tdfh = __tdf[__tdf.Note == \"Homecoming\"]\n",
    "assert( df_worst.iloc[0].OpponentScore == __tdf.loc[(__tdf.iloc[:, OS] - __tdf.iloc[:, IS]).sort_values().index].iloc[-1, OS] ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( df_craziest.iloc[0].OpponentScore == __tdf.loc[(__tdf.iloc[:, OS] + __tdf.iloc[:, IS]).sort_values().index].iloc[-1, OS] ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( df_homecoming_best.iloc[0].IlliniScore == __tdfh.loc[(__tdfh.iloc[:, OS] - __tdfh.iloc[:, IS]).sort_values().index].iloc[0, IS] ), \"Puzzle #1 appears incorrect.\"\n",
    "assert( df_homecoming_best.iloc[0].OpponentScore == __tdfh.loc[(__tdfh.iloc[:, OS] - __tdfh.iloc[:, IS]).sort_values().index].iloc[0, OS] ), \"Puzzle #1 appears incorrect.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Puzzle 2: Are Football Scores Higher in Recent Years?\n",
    "\n",
    "In the Puzzle 1 analysis, most of games you found were recent.  One hypothesis I have is that *\"football games have a higher score recently than they did historically\"*.  Let's see if we have data to support this!\n",
    "\n",
    "Since this data goes all the way back to 1892, let's consider:\n",
    "- \"Recent Games\" to be all games played in the 2000 season to today,\n",
    "- \"Historic Games\" to be all other games (1999 and earlier)\n",
    "\n",
    "Create two DataFrames, `df_recent` and `df_historic`, to store the recent and historic games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent = ...\n",
    "df_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_historic = ...\n",
    "df_historic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average Points Scored\n",
    "\n",
    "Using your two datasets, find the average **total** points scored in recent games and historic games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_avg_score = 0\n",
    "recent_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_avg_score = 0\n",
    "historic_avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ”¬ MicroProject Checkpoint Tests ðŸ”¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 2: Are Football Scores Higher in Recent Years?\n",
    "#\n",
    "# What is this cell?\n",
    "# - This cell contains test cases for the MicroProject. Even though you can modify this\n",
    "#   cell, you should treat it like it's a read-only cell since it will be replaced with\n",
    "#   a fresh version when your code is checked.\n",
    "#\n",
    "# - If this cell runs without any error in the output, you PASSED all test cases!\n",
    "#   We try and make these test cases as useful and complete as possible, but there is\n",
    "#   a chance your code may be incorrect even though you pass the test cases (these\n",
    "#   tests should be seen as a way to give you confidence that code you believe is\n",
    "#   actually correct, not as a robust check to catch all possible errors).\n",
    "#\n",
    "# - If this cell results in any errors, check you previous cells, make changes, and\n",
    "#   RE-RUN your code and then re-run this cell.  Keep repeating this until the cell\n",
    "#   passed with no errors! :)\n",
    "\n",
    "import math\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "assert( 'df_recent' in vars() ), \"Puzzle #2 appears incorrect.\"\n",
    "assert( len(df_recent) > 260 ), \"Puzzle #2 appears incorrect.\"\n",
    "assert( 'df_historic' in vars() ), \"Puzzle #2 appears incorrect.\"\n",
    "assert( len(df_historic) == 1008 ), \"Puzzle #2 appears incorrect.\"\n",
    "assert( len(df_historic) + len(df_recent) == len(df) ), \"Puzzle #2 appears incorrect.\"\n",
    "__tdfg = __tdf.assign( C = __tdf.apply(lambda row: row[\"Season\"] < 2000, axis=1) ).groupby(\"C\").describe()\n",
    "assert( math.isclose(recent_avg_score, sum(__tdfg.loc[False].iloc[[17, 25]])) ), \"Puzzle #2 appears incorrect.\"\n",
    "assert( math.isclose(historic_avg_score, sum(__tdfg.loc[True].iloc[[17, 25]])) ), \"Puzzle #2 appears incorrect.\"\n",
    "\n",
    "print(f\"{tada} Puzzle 2: All Tests Passed! {tada}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Puzzle 3: Create a Bar Chart\n",
    "\n",
    "Finally, let's create a bar chart of the total number of points per year!\n",
    "\n",
    "To do this, set up a DataFrame in the following way:\n",
    "- Each row must be a **season** of football -- not just a single game.\n",
    "- Store the DataFrame, that is ready to be graphed, as `df_points_per_season`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the Graph\n",
    "\n",
    "Once you have a DataFrame all set up, the plot is the easy part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_points_per_season.reset_index().plot.bar(x=\"Season\", y=\"TotalScore\", figsize=(30, 15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ”¬ Microproject Checkpoint Tests ðŸ”¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Puzzle 3: Create a Bar Chart\n",
    "# - This read-only cell contains test cases for the microproject.\n",
    "# - If this cell runs without any error in the output, you PASSED all test cases!\n",
    "# - If this cell results in any errors, check you previous cells, make changes, and RE-RUN your code and then this cell.\n",
    "\n",
    "import math\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "assert( 'df_points_per_season' in vars() ), \"Puzzle #3 appears incorrect.\"\n",
    "assert( math.isclose(df_points_per_season[ df_points_per_season.Season == 2018 ][\"TotalScore\"].iloc[0], 785) ), \"Puzzle #3 appears incorrect.\"\n",
    "assert( math.isclose(df_points_per_season[ df_points_per_season.Season == 2014 ][\"TotalScore\"].iloc[0], 779) ), \"Puzzle #3 appears incorrect.\"\n",
    "\n",
    "print(f\"{tada} Puzzle 3: All Tests Passed! {tada}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission\n",
    "\n",
    "You're almost done!  All you need to do is to commit your lab to GitHub and run the GitHub Actions Grader:\n",
    "\n",
    "1.  âš ï¸ **Make certain to save your work.** âš ï¸ To do this, go to **File => Save All**\n",
    "\n",
    "2.  After you have saved, exit this notebook and return to https://discovery.cs.illinois.edu/microproject/illini-football-scores/ and complete the section **\"Commit and Grade Your Notebook\"**.\n",
    "\n",
    "3. If you see a 100% grade result on your GitHub Action, you've completed this MicroProject! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
